{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a brief attempt to perform Logistic Regression using [Maximum Likelihood Estimation](https://neos-guide.org/content/logit) to estimate model coefficients, using a simple dataset. The emphasis here is not really on the modeling particulars, but to explore using numerical methods to estimate the parameters of a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the well-known [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) for predicting Iris types from petal and sepal dimensions. For this example, we'll be using `petal_length` and `petal_width` to predict whether the Iris is of type *Iris-Virginica* or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, 2:]\n",
    "y = (iris['target']==2).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with scikit-learn first, note that `C` is set to a large number to mimic Logisitc Regression with almost no regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', C=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000.0, class_weight='balanced')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-56.46642413]), array([[8.22989297, 9.80259899]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sklearn = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0],\n",
       "       [ 1, 14]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to fit a Logistic Regression model from scratch. First, we'll need to append a column of ones to our feature matrices to represent the intercept column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1. , 1.6, 0.2],\n",
       "        [1. , 1.5, 0.3],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.9, 2.3],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.2, 2.3],\n",
       "        [1. , 4.5, 1.3],\n",
       "        [1. , 5. , 1.7],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.9, 2.1],\n",
       "        [1. , 5.1, 2. ],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 3.8, 1.1],\n",
       "        [1. , 4.2, 1.5],\n",
       "        [1. , 3.7, 1. ],\n",
       "        [1. , 6. , 1.8],\n",
       "        [1. , 1. , 0.2],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.7, 2.5],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 3.9, 1.1],\n",
       "        [1. , 4.9, 1.8],\n",
       "        [1. , 6.6, 2.1],\n",
       "        [1. , 6.7, 2. ],\n",
       "        [1. , 1.3, 0.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 3.3, 1. ],\n",
       "        [1. , 4.1, 1.3],\n",
       "        [1. , 4.4, 1.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 1.9, 0.4],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 1.7, 0.3],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.7, 1.6],\n",
       "        [1. , 5. , 1.9],\n",
       "        [1. , 6.4, 2. ],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 5.6, 2.1],\n",
       "        [1. , 1.4, 0.1],\n",
       "        [1. , 5. , 1.5],\n",
       "        [1. , 4.5, 1.6],\n",
       "        [1. , 6. , 2.5],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 1.6, 0.4],\n",
       "        [1. , 6.1, 2.3],\n",
       "        [1. , 1.1, 0.1],\n",
       "        [1. , 5. , 2. ],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.8, 1.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.5, 1.8],\n",
       "        [1. , 4.6, 1.5],\n",
       "        [1. , 5.6, 1.8],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 1.2, 0.2],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 4.7, 1.4],\n",
       "        [1. , 3.6, 1.3],\n",
       "        [1. , 1.6, 0.6],\n",
       "        [1. , 6.1, 2.5],\n",
       "        [1. , 4. , 1. ],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 5.6, 2.4],\n",
       "        [1. , 4.9, 2. ],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 5.7, 2.3],\n",
       "        [1. , 1.9, 0.2],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 6.9, 2.3],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 5.3, 2.3],\n",
       "        [1. , 5.1, 1.8],\n",
       "        [1. , 4.3, 1.3],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 6.7, 2.2],\n",
       "        [1. , 5.5, 2.1],\n",
       "        [1. , 1.3, 0.3],\n",
       "        [1. , 5.1, 1.6],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 5.8, 1.6],\n",
       "        [1. , 5.1, 2.4],\n",
       "        [1. , 4.9, 1.5],\n",
       "        [1. , 5.1, 1.9],\n",
       "        [1. , 4.2, 1.2],\n",
       "        [1. , 5.7, 2.1],\n",
       "        [1. , 6.1, 1.9],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.1, 1.3],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 4.1, 1. ],\n",
       "        [1. , 4.2, 1.3],\n",
       "        [1. , 5.4, 2.1],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 5.1, 1.9],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 4.7, 1.2],\n",
       "        [1. , 1.7, 0.4],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 4.4, 1.2]]), array([[1. , 6.3, 1.8],\n",
       "        [1. , 4.6, 1.4],\n",
       "        [1. , 5.1, 1.5],\n",
       "        [1. , 1.7, 0.2],\n",
       "        [1. , 4.9, 1.8],\n",
       "        [1. , 5.6, 2.4],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 3.9, 1.4],\n",
       "        [1. , 5.1, 2.3],\n",
       "        [1. , 1.4, 0.1],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.8, 1.8],\n",
       "        [1. , 5.5, 1.8],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.4, 2.3],\n",
       "        [1. , 4.7, 1.5],\n",
       "        [1. , 3.3, 1. ],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 4.7, 1.4],\n",
       "        [1. , 4. , 1.2],\n",
       "        [1. , 5.8, 2.2],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 1.2, 0.2],\n",
       "        [1. , 4.3, 1.3],\n",
       "        [1. , 3.9, 1.2],\n",
       "        [1. , 4.4, 1.3],\n",
       "        [1. , 4.6, 1.3],\n",
       "        [1. , 5.6, 1.4],\n",
       "        [1. , 5.6, 2.2],\n",
       "        [1. , 1.7, 0.5],\n",
       "        [1. , 4.2, 1.3],\n",
       "        [1. , 5.3, 1.9],\n",
       "        [1. , 3. , 1.1],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 3.5, 1. ],\n",
       "        [1. , 1.3, 0.3],\n",
       "        [1. , 3.5, 1. ],\n",
       "        [1. , 4.9, 1.5],\n",
       "        [1. , 4.4, 1.4],\n",
       "        [1. , 4.5, 1.7],\n",
       "        [1. , 5.2, 2. ],\n",
       "        [1. , 4.5, 1.5]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2, X_test2 = np.concatenate([np.ones([len(X_train), 1]), X_train], axis=1), np.concatenate([np.ones([len(X_test), 1]), X_test], axis=1)\n",
    "X_train2, X_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to find the set of Logistic Regression coefficients maximizing the Log Likelihood for Logistic Regression ([see bottom Page 2 and top Page 3 for its gradient](https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf)). We will specifically use gradient ascent (not descent, because we are maximizing) to iteratively determine the coefficients (`betas`), with a starting guess of all three coefficients (intercept plus two feature weights) being zero. Note that this will take many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.zeros(3)\n",
    "err = 9999\n",
    "learn = 2\n",
    "\n",
    "while err > 1e-9:\n",
    "    step = np.dot(X_train2.T, y_train - (1/(1 + np.exp(-np.dot(X_train2, betas)))))\n",
    "    betas_new = betas + learn * (step/len(X_train2))\n",
    "    err = np.linalg.norm(betas_new - betas)\n",
    "    betas = betas_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-55.08588652   7.38303664  10.99317481]\n",
      "[-56.46642413] [[8.22989297 9.80259899]]\n"
     ]
    }
   ],
   "source": [
    "print(betas)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the coefficients we've estimated by maximizing the Log Likelikhood function using gradient ascent are a bit off from what scikit-learn. This result could be due to there not being completely zero regularization in the model `lr` that we found using scikit-learn or the highly optimized algorithms scikit-learn uses compared to our barebones implementation which besides being slow, also hasn't really optimized for learning rate either.\n",
    "\n",
    "Let's see how good our predictions compare now (note that we are using the logistic (sigmoid) function and mapping the resulting probability of being a \"positive\" -- the *Iris-Virginica* type to the class prediction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(x >= .5) for x in (1/(1 + np.exp(-np.dot(X_test2, betas))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred==y_pred_sklearn) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        30\n",
      "           1       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.97      0.93      0.95        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0],\n",
       "       [ 2, 13]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks as if we are off by one false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now clean things up and make a class out of what we did above (Note that I've created a method here to compute the [sigmoid function](https://sebastianraschka.com/faq/docs/logistic-why-sigmoid.html) for a given set of observations and coefficients (betas) which denote the predicted conditional positive class probability given the observations' feature values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    def __init__(self, fit_intercept=True, learning_rate=0.1, stopping=1e-4):\n",
    "        # constructor, either fit w/ intercept or not, can specify learning rate and gradient ascent stopping criterion as well\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.learning_rate = learning_rate\n",
    "        self.stopping = stopping\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        # sigmoid function to make predictions\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        # add column of ones (as intercept column) to X\n",
    "        return np.concatenate([np.ones([len(X), 1]), X], axis=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # find the weights using gradient ascent on the log likelihood\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "        \n",
    "        betas = np.zeros(len(X.T))\n",
    "        step = 99999 * np.ones(len(X.T))\n",
    "        \n",
    "        while np.linalg.norm(step) > self.stopping:\n",
    "            step = self.learning_rate * (1/len(X)) * np.dot(X.T, y-self.sigmoid(np.dot(X, betas)))\n",
    "            betas += step\n",
    "        \n",
    "        self.betas = betas\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # assign observations to positive class probability using sigmoid function\n",
    "        return self.sigmoid(np.dot(self.add_intercept(X), betas)) if self.fit_intercept else self.sigmoid(np.dot(X, betas))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # can add a threshold argument but just use the default decision threshold of 0.5\n",
    "        return self.predict_proba(X) >= 0.5\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # accuracy score\n",
    "        return sum(self.predict(X) == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lr = Logistic_Regression(learning_rate=2, stopping=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_my_lr = my_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_my_lr==y_pred_sklearn) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0],\n",
       "       [ 2, 13]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-55.0462033    7.37615574  10.98984993]\n",
      "[-56.46642413] [[8.22989297 9.80259899]]\n"
     ]
    }
   ],
   "source": [
    "print(my_lr.betas)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks as if this does about what we expect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
