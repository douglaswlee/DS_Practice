{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a brief attempt to perform Logistic Regression using [Maximum Likelihood Estimation](https://neos-guide.org/content/logit) to estimate model coefficients, using a simple dataset. The emphasis here is not really on the modeling particulars, but to explore using numerical methods to estimate the parameters of a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the well-known [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) for predicting Iris types from petal and sepal dimensions. For this example, we'll be using `petal_length` and `petal_width` to predict whether the Iris is of type *Iris-Virginica* or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, 2:]\n",
    "y = (iris['target']==2).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with scikit-learn first, note that `C` is set to a large number to mimic Logisitc Regression with almost no regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', C=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-56.4120616]), array([[8.21998445, 9.79924428]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sklearn = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        30\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0],\n",
       "       [ 1, 14]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to fit a Logistic Regression model from scratch. First, we'll need to append a column of ones to our feature matrices to represent the intercept column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1. , 1.6, 0.2],\n",
       "        [1. , 1.5, 0.3],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.9, 2.3],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.2, 2.3],\n",
       "        [1. , 4.5, 1.3],\n",
       "        [1. , 5. , 1.7],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.9, 2.1],\n",
       "        [1. , 5.1, 2. ],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 3.8, 1.1],\n",
       "        [1. , 4.2, 1.5],\n",
       "        [1. , 3.7, 1. ],\n",
       "        [1. , 6. , 1.8],\n",
       "        [1. , 1. , 0.2],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.7, 2.5],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 3.9, 1.1],\n",
       "        [1. , 4.9, 1.8],\n",
       "        [1. , 6.6, 2.1],\n",
       "        [1. , 6.7, 2. ],\n",
       "        [1. , 1.3, 0.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 3.3, 1. ],\n",
       "        [1. , 4.1, 1.3],\n",
       "        [1. , 4.4, 1.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 1.9, 0.4],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 1.7, 0.3],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.7, 1.6],\n",
       "        [1. , 5. , 1.9],\n",
       "        [1. , 6.4, 2. ],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 5.6, 2.1],\n",
       "        [1. , 1.4, 0.1],\n",
       "        [1. , 5. , 1.5],\n",
       "        [1. , 4.5, 1.6],\n",
       "        [1. , 6. , 2.5],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 1.6, 0.4],\n",
       "        [1. , 6.1, 2.3],\n",
       "        [1. , 1.1, 0.1],\n",
       "        [1. , 5. , 2. ],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.8, 1.4],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.5, 1.8],\n",
       "        [1. , 4.6, 1.5],\n",
       "        [1. , 5.6, 1.8],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 1.2, 0.2],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 4.7, 1.4],\n",
       "        [1. , 3.6, 1.3],\n",
       "        [1. , 1.6, 0.6],\n",
       "        [1. , 6.1, 2.5],\n",
       "        [1. , 4. , 1. ],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 5.6, 2.4],\n",
       "        [1. , 4.9, 2. ],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 5.7, 2.3],\n",
       "        [1. , 1.9, 0.2],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 6.9, 2.3],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 5.3, 2.3],\n",
       "        [1. , 5.1, 1.8],\n",
       "        [1. , 4.3, 1.3],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 6.7, 2.2],\n",
       "        [1. , 5.5, 2.1],\n",
       "        [1. , 1.3, 0.3],\n",
       "        [1. , 5.1, 1.6],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 5.8, 1.6],\n",
       "        [1. , 5.1, 2.4],\n",
       "        [1. , 4.9, 1.5],\n",
       "        [1. , 5.1, 1.9],\n",
       "        [1. , 4.2, 1.2],\n",
       "        [1. , 5.7, 2.1],\n",
       "        [1. , 6.1, 1.9],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.4, 0.3],\n",
       "        [1. , 4.1, 1.3],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 4.1, 1. ],\n",
       "        [1. , 4.2, 1.3],\n",
       "        [1. , 5.4, 2.1],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 5.1, 1.9],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 4.7, 1.2],\n",
       "        [1. , 1.7, 0.4],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 4.4, 1.2]]), array([[1. , 6.3, 1.8],\n",
       "        [1. , 4.6, 1.4],\n",
       "        [1. , 5.1, 1.5],\n",
       "        [1. , 1.7, 0.2],\n",
       "        [1. , 4.9, 1.8],\n",
       "        [1. , 5.6, 2.4],\n",
       "        [1. , 1.3, 0.2],\n",
       "        [1. , 4. , 1.3],\n",
       "        [1. , 3.9, 1.4],\n",
       "        [1. , 5.1, 2.3],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 1.4, 0.2],\n",
       "        [1. , 5.8, 1.8],\n",
       "        [1. , 5.5, 1.8],\n",
       "        [1. , 1.6, 0.2],\n",
       "        [1. , 5.4, 2.3],\n",
       "        [1. , 4.7, 1.5],\n",
       "        [1. , 3.3, 1. ],\n",
       "        [1. , 1.5, 0.2],\n",
       "        [1. , 1.5, 0.1],\n",
       "        [1. , 1.5, 0.4],\n",
       "        [1. , 4.7, 1.4],\n",
       "        [1. , 4. , 1.2],\n",
       "        [1. , 5.8, 2.2],\n",
       "        [1. , 4.8, 1.8],\n",
       "        [1. , 1.2, 0.2],\n",
       "        [1. , 4.3, 1.3],\n",
       "        [1. , 3.9, 1.2],\n",
       "        [1. , 4.4, 1.3],\n",
       "        [1. , 4.6, 1.3],\n",
       "        [1. , 5.6, 1.4],\n",
       "        [1. , 5.6, 2.2],\n",
       "        [1. , 1.7, 0.5],\n",
       "        [1. , 4.2, 1.3],\n",
       "        [1. , 5.3, 1.9],\n",
       "        [1. , 3. , 1.1],\n",
       "        [1. , 4.5, 1.5],\n",
       "        [1. , 3.5, 1. ],\n",
       "        [1. , 1.3, 0.3],\n",
       "        [1. , 3.5, 1. ],\n",
       "        [1. , 4.9, 1.5],\n",
       "        [1. , 4.4, 1.4],\n",
       "        [1. , 4.5, 1.7],\n",
       "        [1. , 5.2, 2. ],\n",
       "        [1. , 4.5, 1.5]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2, X_test2 = np.concatenate([np.ones([len(X_train), 1]), X_train], axis=1), np.concatenate([np.ones([len(X_test), 1]), X_test], axis=1)\n",
    "X_train2, X_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to find the set of Logistic Regression coefficients maximizing the Log Likelihood for Logistic Regression ([see bottom Page 2 and top Page 3 for its gradient](https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf)). We will specifically use gradient ascent (not descent, because we are maximizing) to iteratively determine the coefficients (`betas`), with a starting guess of all three coefficients (intercept plus two feature weights) being zero. Note that this will take many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.zeros(3)\n",
    "err = 9999\n",
    "learn = 1e-3\n",
    "\n",
    "while err > 1e-6:\n",
    "    step = np.dot(X_train2.T, y_train - (1/(1 + np.exp(-np.dot(X_train2, betas)))))\n",
    "    betas_new = betas + learn * step\n",
    "    err = np.linalg.norm(betas_new - betas)\n",
    "    betas = betas_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-54.3444985    7.2544039   10.93128676]\n",
      "[-56.4120616] [[8.21998445 9.79924428]]\n"
     ]
    }
   ],
   "source": [
    "print(betas)\n",
    "print(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the coefficients we've estimated by maximizing the Log Likelikhood function using gradient ascent are a bit off from what scikit-learn. This result could be due to there not being completely zero regularization in the model `lr` that we found using scikit-learn or the highly optimized algorithms scikit-learn uses compared to our barebones implementation which besides being slow, also hasn't really optimized for learning rate either.\n",
    "\n",
    "Let's see how good our predictions compare now (note that we are using the logistic (sigmoid) function and mapping the resulting probability of being a \"positive\" -- the *Iris-Virginica* type to the class prediction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(x >= .5) for x in (1/(1 + np.exp(-np.dot(X_test2, betas))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred==y_pred_sklearn) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        30\n",
      "          1       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.96      0.96      0.95        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  0],\n",
       "       [ 2, 13]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks as if we are off by one false negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
